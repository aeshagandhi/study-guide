# ğŸ§  IDS 702 Midterm Comprehensive Study Guide

---

## ğŸ“ Unit 1: Statistics Fundamentals

---

### 1ï¸âƒ£ Population vs. Sample
- **Population**: the entire group of interest (e.g., all Duke students).  
- **Sample**: a subset used to estimate population parameters.  
- **Goal**: draw *inferences* about the population using the sample.  
- Sampling introduces **sampling variability** â€” estimates differ across samples.

---

### 2ï¸âƒ£ Inference vs. Prediction
| Type | Goal | Example |
|------|------|----------|
| **Inference** | Understand relationships / test hypotheses | Does smoking affect heart disease risk? |
| **Prediction** | Forecast outcomes | Predict next yearâ€™s COâ‚‚ emissions |

---

### 3ï¸âƒ£ Probability Concepts
- **Random Variable (RV):** numerical outcome of a random process.  
- **PMF/PDF:** Probability Mass (discrete) or Density (continuous) function.  
- **Expected Value (E[X]):** long-run mean of RV.  
- **Variance (Var[X]):** spread around the mean.  
- **Joint Probability:** P(A and B)  
- **Conditional Probability:** P(A | B) = P(A and B) / P(B)  
- **Mutually Exclusive:** if A and B cannot occur together, then P(A and B)=0.

---

### 4ï¸âƒ£ Common Distributions
- **Binomial:** number of successes in *n* independent Bernoulli trials.  
  Parameters â†’ n (trials), p (probability of success).  
- **Normal:** continuous, symmetric â€œbell curve,â€ defined by mean Î¼ and variance ÏƒÂ².  
- For large n, Binomial â‰ˆ Normal with Î¼ = np, ÏƒÂ² = np(1âˆ’p).

### ğŸ“Š Summary Table

| Distribution | Type | Parameters | Support (Range) | Expected Value (E[X]) | Variance (Var[X]) | Typical Use Case |
|---------------|-------|-------------|------------------|------------------------|-------------------|------------------|
| **Bernoulli** | Discrete | p âˆˆ [0,1] | x âˆˆ {0,1} | p | p(1 âˆ’ p) | Single binary outcome (success/failure) |
| **Binomial** | Discrete | n (trials), p (success prob) | x âˆˆ {0,1,â€¦,n} | np | np(1 âˆ’ p) | # of successes in n independent trials |
| **Geometric** | Discrete | p âˆˆ (0,1) | x âˆˆ {1,2,3,â€¦} | 1/p | (1 âˆ’ p)/pÂ² | # of trials until first success |
| **Poisson** | Discrete | Î» > 0 | x âˆˆ {0,1,2,â€¦} | Î» | Î» | Count of events in fixed time/space |
| **Uniform (Discrete)** | Discrete | k equally likely values | x âˆˆ {1,â€¦,k} | (k + 1)/2 | (kÂ² âˆ’ 1)/12 | Equal likelihood categorical outcomes |
| **Uniform (Continuous)** | Continuous | a, b (bounds) | x âˆˆ [a,b] | (a + b)/2 | (b âˆ’ a)Â² / 12 | Random real number in interval [a,b] |
| **Normal (Gaussian)** | Continuous | Î¼ (mean), ÏƒÂ² (variance) | x âˆˆ â„ | Î¼ | ÏƒÂ² | Many natural phenomena; CLT limit |
| **Exponential** | Continuous | Î» > 0 | x â‰¥ 0 | 1/Î» | 1/Î»Â² | Time between Poisson events |
| **Gamma** | Continuous | Î± (shape), Î² (rate) | x â‰¥ 0 | Î±/Î² | Î±/Î²Â² | Sum of exponential variables |
| **Beta** | Continuous | Î± > 0, Î² > 0 | x âˆˆ [0,1] | Î±/(Î± + Î²) | (Î±Î²)/[(Î± + Î²)Â²(Î± + Î² + 1)] | Probabilities constrained between 0 and 1 |
| **Chi-Square (Ï‡Â²)** | Continuous | k (degrees of freedom) | x â‰¥ 0 | k | 2k | Variance estimates, hypothesis tests |
| **t (Studentâ€™s t)** | Continuous | Î½ (degrees of freedom) | x âˆˆ â„ | 0 | Î½/(Î½ âˆ’ 2), Î½>2 | Mean estimates with small samples |
| **F** | Continuous | dâ‚, dâ‚‚ (degrees of freedom) | x â‰¥ 0 | dâ‚‚/(dâ‚‚ âˆ’ 2) (if dâ‚‚>2) | *Var depends on df* | Comparing variances, ANOVA, regression F-tests |

---

---

### 5ï¸âƒ£ Sampling Distributions & Central Limit Theorem
- **Sampling Distribution:** distribution of a statistic (e.g., sample mean) over many samples.  
- **CLT:** for large n, the sample mean is approximately normal:  
  `XÌ„ ~ Normal(Î¼, ÏƒÂ²/n)`  
- **Standard deviation** = spread of data; **Standard error (SE)** = spread of statistic (âˆ 1/âˆšn).

---

### 6ï¸âƒ£ Maximum Likelihood Estimation (MLE)
1. Write likelihood L(Î¸)=P(data | Î¸)  
2. Take log â†’ log-likelihood  
3. Differentiate w.r.t Î¸, set = 0  
4. Solve for Î¸Ì‚ (the MLE)

**Idea:** choose parameters that make observed data *most likely*.  
MLEs are consistent, unbiased (as nâ†’âˆ), and efficient.

---

### 7ï¸âƒ£ Confidence Intervals (CI)
- **Interpretation:** Repeating sampling many times, about 95 % of intervals contain the true parameter.  
- **Width increases** when:
  - Variability â†‘  
  - Confidence level â†‘  
  - Sample size â†“  
- **Analytical CI:** uses formulas (e.g., `xÌ„ Â± z * s/âˆšn`)  
- **Bootstrap CI:** obtained via resampling.

---

### 8ï¸âƒ£ Hypothesis Testing
1. Set up Hâ‚€ (null) and Hâ‚ (alternative).  
2. Compute test statistic (t, z, F, Ï‡Â²).  
3. Compute p-value = Pr(data or more extreme | Hâ‚€).  
4. Reject Hâ‚€ if p < Î± (usually 0.05).  

**Parametric:** assumes known distribution (t-test).  
**Simulation-based:** permutation or bootstrap test.  
**Paired vs Independent:** within- vs across-sample comparisons.

---

## ğŸ“ Unit 2: Linear Regression

---

### 1ï¸âƒ£ Simple and Multiple Linear Regression
**Model:**  
`yáµ¢ = Î²â‚€ + Î²â‚xáµ¢â‚ + Î²â‚‚xáµ¢â‚‚ + â€¦ + Î²â‚šxáµ¢â‚š + Îµáµ¢`

**Goal:** estimate coefficients Î² that minimize  
`RSS = Î£(yáµ¢ âˆ’ Å·áµ¢)Â²`

**Interpretation:**  
- Î²Ì‚â±¼ = expected change in Y for a one-unit change in Xâ±¼, holding others constant.  
- p-value tests if Î²â±¼ significantly differs from 0.  
- CI gives a range of plausible slopes.

---

### 2ï¸âƒ£ Matrix Notation
`Y = XÎ² + Îµ`  
- Y: nÃ—1 vector of responses  
- X: nÃ—p design matrix (first column = 1s for intercept)  
- Î²: pÃ—1 vector of coefficients  
- Îµ: nÃ—1 vector of errors  

**OLS solution:** `Î²Ì‚ = (Xáµ€X)â»Â¹Xáµ€Y`

---

### 3ï¸âƒ£ Categorical Predictors
- Represent each level (except one) with dummy variables (0/1).  
- Reference = baseline category.  
- For K levels â†’ K âˆ’ 1 dummies.  
- Avoid K dummies â†’ perfect multicollinearity.  

**Interpretation:** each coefficient = difference from reference group mean (Y).

---

### 4ï¸âƒ£ Interaction Terms
Model:  
`Y = Î²â‚€ + Î²â‚Xâ‚ + Î²â‚‚Xâ‚‚ + Î²â‚ƒ(Xâ‚Â·Xâ‚‚) + Îµ`

- The term Î²â‚ƒ(Xâ‚Â·Xâ‚‚) allows the effect of Xâ‚ to depend on Xâ‚‚.  
- If Î²â‚ƒ â‰  0 â†’ interaction significant.  
- Example: effect of dosage on anxiety differs by age group.  
- Use when theory or EDA suggest different slopes by groups.

---

### 5ï¸âƒ£ Model Assessment
**Coefficient of Determination (RÂ²):**  
`RÂ² = 1 âˆ’ (RSS / TSS)` â†’ proportion of variance in Y explained by model.

**Adjusted RÂ²:**  
`RÂ²_adj = 1 âˆ’ (1 âˆ’ RÂ²) * ((n âˆ’ 1)/(n âˆ’ p âˆ’ 1))`  
Penalizes extra predictors.

**Interpretation:**  
- High RÂ² means more variance explained but not necessarily better fit.  
- Use Adjusted RÂ² for comparing models with different numbers of predictors.

---

### 6ï¸âƒ£ Regression Assumptions

| Assumption | Diagnostic Plot | Expected | Violation / Fix |
|-------------|----------------|-----------|----------------|
| **Linearity** | Residuals vs Fitted | Random scatter | Add polynomial terms or transform X |
| **Independence** | Residuals vs Index | No trend | Account for design or use mixed models |
| **Normality** | Q-Q plot | Points â‰ˆ 45Â° line | Transform Y or use robust methods |
| **Equal Variance** | Residuals vs Fitted | Constant spread | Transform Y or weighted LS |

Violating assumptions affects inference (p-values, CIs) more than prediction.

---

### 7ï¸âƒ£ F-Test (Overall Model)
Tests if model explains any variation in Y.

- **Null:** Î²â‚ = Î²â‚‚ = â€¦ = Î²â‚š = 0  
- **Statistic:** `F = (SSR/p) / (SSE/(n âˆ’ p âˆ’ 1))`  
- **Decision:** large F â†’ reject Hâ‚€ â†’ at least one predictor significant.

---

### 8ï¸âƒ£ Nested F-Test (Partial F-Test)
Compares two models:

- Reduced model âŠ‚ Full model  
- Tests if added predictors significantly improve fit.

**Formula:**  
`F = ((RSS_R âˆ’ RSS_F)/(p_F âˆ’ p_R)) / (RSS_F/(n âˆ’ p_F))`

**Interpretation:**  
Reject Hâ‚€ if F large â†’ extra predictors help.

**Example (R):**
```r
anova(reduced_model, full_model)
```

---

### 9ï¸âƒ£ Influential Points and Diagnostics

#### a. Leverage
- **Definition:** Measures how far a pointâ€™s predictor values are from the average of all predictors.  
- **Formula (conceptually):**  
  `háµ¢áµ¢ = xáµ¢áµ€ (Xáµ€X)â»Â¹ xáµ¢`  
- **Interpretation:**  
  - High leverage â†’ point is far out in X-space.  
  - These points have the *potential* to influence the regression line, even if their residual is small.  
  - The average leverage = (p + 1)/n, where p = number of predictors.  
- **Guideline:**  
  - háµ¢áµ¢ > 2Ã—average â†’ moderately high leverage  
  - háµ¢áµ¢ > 3Ã—average â†’ very high leverage  

---

#### b. Residuals vs Leverage Plot
- **Purpose:** identify points that combine *high leverage* (far in X-space) and *large residuals* (far in Y).  
- **Axes:**  
  - x-axis â†’ leverage  
  - y-axis â†’ studentized (standardized) residuals  
- **Interpretation:**  
  - Points near the center: typical observations.  
  - Points far right â†’ high leverage.  
  - Points far up/down â†’ large residuals (Y-outliers).  
  - Points far right *and* outside the Cookâ€™s-distance contours â†’ **influential**.  

---

#### c. Cookâ€™s Distance
- **Definition:** Quantifies how much the entire fitted model would change if an observation were removed.  
- **Approximate formula:**  
  `Dáµ¢ = (eáµ¢Â² / (pÂ·MSE)) * (háµ¢áµ¢ / (1 âˆ’ háµ¢áµ¢)Â²)`  
- **Interpretation:**  
  - Dáµ¢ â‰ˆ 0 â†’ no influence  
  - Dáµ¢ > 0.5 â†’ potentially influential  
  - Dáµ¢ > 1 â†’ highly influential  
- **What to do:**  
  - Verify data entry.  
  - Fit the model with and without the observation â€” if coefficients change greatly, document or justify exclusion.  

---

#### d. Variance Inflation Factor (VIF)
- **Definition:** Measures how much a coefficientâ€™s variance is inflated due to collinearity.  
- **Formula:**  
  `VIFâ±¼ = 1 / (1 âˆ’ Râ±¼Â²)`  
  where Râ±¼Â² = RÂ² from regressing predictor Xâ±¼ on all other Xâ€™s.  
- **Guidelines:**  
  - VIF â‰ˆ 1 â†’ no correlation  
  - VIF > 5 â†’ moderate multicollinearity  
  - VIF > 10 â†’ serious multicollinearity  
- **Fixes:**  
  - Remove redundant predictors.  
  - Combine correlated features.  
  - Use ridge regression or PCA if all predictors are important.  

---

### ğŸ”Ÿ Multicollinearity

- **Definition:** Two or more predictors are highly linearly related.  
- **Why it matters:** makes coefficients unstable and inflates their standard errors.  

**Consequences**
- Coefficient signs may flip unexpectedly.  
- Large SE â†’ insignificant p-values even for important variables.  
- Difficult to isolate the individual effect of predictors.  

**Detection**
- High pairwise correlations (|r| > 0.9).  
- High VIFs (see above).  

**Solutions**
- Drop or combine correlated variables.  
- Standardize predictors to improve interpretability.  
- Apply regularization (ridge / lasso).  
- In some cases, accept collinearity if predictors are theoretically essential.  

---

### ğŸ“Š Diagnostic Plot Summary

| Plot | What It Checks | Expected Pattern | Violation Indicates |
|------|----------------|------------------|----------------------|
| **Residuals vs Fitted** | Linearity + Equal Variance | Random scatter around 0 | Curved pattern â†’ nonlinearity; Funnel â†’ heteroscedasticity |
| **Qâ€“Q Plot** | Normality of residuals | Points â‰ˆ 45Â° line | Skewed or heavy tails â†’ non-normal errors |
| **Scaleâ€“Location** | Homoscedasticity | Horizontal band | Funnel â†’ non-constant variance |
| **Residuals vs Leverage** | Influence / Outliers | Most within Cookâ€™s D < 1 | Outliers or high leverage points |

---

### ğŸ§  Concept and Test Relationships

| Concept | Purpose | Formula / Key Idea |
|----------|----------|--------------------|
| **t-Test** | Tests a single coefficient | `t = Î²Ì‚ / SE(Î²Ì‚)` |
| **F-Test (overall)** | Tests if *any* predictor matters | `F = MSR / MSE` |
| **Nested F-Test** | Tests if new predictors improve model | `F = ((RSS_R âˆ’ RSS_F)/(p_F âˆ’ p_R)) / (RSS_F/(n âˆ’ p_F))` |
| **Leverage** | Measures distance in X-space | `háµ¢áµ¢` |
| **Cookâ€™s Distance** | Measures influence | `Dáµ¢` |
| **VIF** | Detects multicollinearity | `1 / (1 âˆ’ RÂ²)` |

---

### ğŸ§¾ Practical Exam Tips

**Focus on interpretation rather than memorization**
- Understand what each test or plot *tells you*.  
- Be able to read regression output:  
  - Which coefficients are significant?  
  - What is the practical meaning of Î²Ì‚â±¼?  
  - Are assumptions violated (based on residual plots)?  

**Key ideas to recall**
- **CLT:** sampling distributions become approximately normal.  
- **OLS:** minimizes RSS â†’ best linear unbiased estimates (BLUE).  
- **F-ratio:** compares explained vs. unexplained variance.  
- **Nested F-test:** compares two models directly.  

**Quick thresholds**
- p < 0.05 â†’ statistically significant.  
- Dáµ¢ > 1 â†’ potentially influential.  
- VIF > 5 â†’ collinearity concern.  

**Strategy for short-answer questions**
1. Identify model type (SLR / MLR).  
2. Recall what the coefficient or statistic represents.  
3. State direction (+/â€“) and magnitude of effect.  
4. Comment on assumption checks (plots).  

**Remember:**  
- Interpret regression *in context* (e.g., units, variables).  
- A large RÂ² doesnâ€™t mean the model is valid â†’ check residual diagnostics.  
- High leverage â‰  automatically bad â€” only problematic if residuals are large too.  

---

### ğŸ§­ Final Summary â€” Top Exam Takeaways

1. **Understand the 4 regression assumptions** (linearity, independence, normality, homoscedasticity).  
2. **OLS minimizes squared residuals** to estimate Î²â€™s.  
3. **t-tests** check individual predictors; **F-tests** check groups.  
4. **Nested F-tests** compare reduced vs full models.  
5. **RÂ² vs Adjusted RÂ²:** adjusted penalizes extra predictors.  
6. **Leverage + Cookâ€™s Distance:** detect influence.  
7. **VIF:** detect multicollinearity.  
8. **Confidence Intervals:** interpretation = â€œif repeated many times, 95 % of CIs would contain Î².â€  
9. **p-values:** probability of data (or more extreme) given Hâ‚€ true.  
10. **Always explain in plain language** â€” what does the coefficient or diagnostic mean in the studyâ€™s context?  

---


