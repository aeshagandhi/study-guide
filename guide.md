# ğŸ§  IDS 702 Midterm Comprehensive Study Guide

---

## ğŸ“ Unit 1: Statistics Fundamentals

---

### 1ï¸âƒ£ Population vs. Sample
- **Population**: the entire group of interest (e.g., all Duke students).  
- **Sample**: a subset used to estimate population parameters.  
- **Goal**: draw *inferences* about the population using the sample.  
- Sampling introduces **sampling variability** â€” estimates differ across samples.

---

### 2ï¸âƒ£ Inference vs. Prediction
| Type | Goal | Example |
|------|------|----------|
| **Inference** | Understand relationships / test hypotheses | Does smoking affect heart disease risk? |
| **Prediction** | Forecast outcomes | Predict next yearâ€™s COâ‚‚ emissions |

---

### 3ï¸âƒ£ Probability Concepts
- **Random Variable (RV):** numerical outcome of a random process.  
- **PMF/PDF:** Probability Mass (discrete) or Density (continuous) function.  
- **Expected Value (E[X]):** long-run mean of RV.  
- **Variance (Var[X]):** spread around the mean.  
- **Joint Probability:** P(A and B)  
- **Conditional Probability:** P(A | B) = P(A and B) / P(B)  
- **Mutually Exclusive:** if A and B cannot occur together, then P(A and B)=0.

---

### 4ï¸âƒ£ Common Distributions
- **Binomial:** number of successes in *n* independent Bernoulli trials.  
  Parameters â†’ n (trials), p (probability of success).  
- **Normal:** continuous, symmetric â€œbell curve,â€ defined by mean Î¼ and variance ÏƒÂ².  
- For large n, Binomial â‰ˆ Normal with Î¼ = np, ÏƒÂ² = np(1âˆ’p).

### ğŸ“Š Summary Table

| Distribution | Type | Parameters | Support (Range) | Expected Value (E[X]) | Variance (Var[X]) | Typical Use Case |
|---------------|-------|-------------|------------------|------------------------|-------------------|------------------|
| **Bernoulli** | Discrete | p âˆˆ [0,1] | x âˆˆ {0,1} | p | p(1 âˆ’ p) | Single binary outcome (success/failure) |
| **Binomial** | Discrete | n (trials), p (success prob) | x âˆˆ {0,1,â€¦,n} | np | np(1 âˆ’ p) | # of successes in n independent trials |
| **Geometric** | Discrete | p âˆˆ (0,1) | x âˆˆ {1,2,3,â€¦} | 1/p | (1 âˆ’ p)/pÂ² | # of trials until first success |
| **Poisson** | Discrete | Î» > 0 | x âˆˆ {0,1,2,â€¦} | Î» | Î» | Count of events in fixed time/space |
| **Uniform (Discrete)** | Discrete | k equally likely values | x âˆˆ {1,â€¦,k} | (k + 1)/2 | (kÂ² âˆ’ 1)/12 | Equal likelihood categorical outcomes |
| **Uniform (Continuous)** | Continuous | a, b (bounds) | x âˆˆ [a,b] | (a + b)/2 | (b âˆ’ a)Â² / 12 | Random real number in interval [a,b] |
| **Normal (Gaussian)** | Continuous | Î¼ (mean), ÏƒÂ² (variance) | x âˆˆ â„ | Î¼ | ÏƒÂ² | Many natural phenomena; CLT limit |
| **Exponential** | Continuous | Î» > 0 | x â‰¥ 0 | 1/Î» | 1/Î»Â² | Time between Poisson events |
| **Gamma** | Continuous | Î± (shape), Î² (rate) | x â‰¥ 0 | Î±/Î² | Î±/Î²Â² | Sum of exponential variables |
| **Beta** | Continuous | Î± > 0, Î² > 0 | x âˆˆ [0,1] | Î±/(Î± + Î²) | (Î±Î²)/[(Î± + Î²)Â²(Î± + Î² + 1)] | Probabilities constrained between 0 and 1 |
| **Chi-Square (Ï‡Â²)** | Continuous | k (degrees of freedom) | x â‰¥ 0 | k | 2k | Variance estimates, hypothesis tests |
| **t (Studentâ€™s t)** | Continuous | Î½ (degrees of freedom) | x âˆˆ â„ | 0 | Î½/(Î½ âˆ’ 2), Î½>2 | Mean estimates with small samples |
| **F** | Continuous | dâ‚, dâ‚‚ (degrees of freedom) | x â‰¥ 0 | dâ‚‚/(dâ‚‚ âˆ’ 2) (if dâ‚‚>2) | *Var depends on df* | Comparing variances, ANOVA, regression F-tests |

---

---

### 5ï¸âƒ£ Sampling Distributions & Central Limit Theorem
- **Sampling Distribution:** distribution of a statistic (e.g., sample mean) over many samples.  
- **CLT:** for large n, the sample mean is approximately normal:  
  `XÌ„ ~ Normal(Î¼, ÏƒÂ²/n)`  
- **Standard deviation** = spread of data; **Standard error (SE)** = spread of statistic (âˆ 1/âˆšn).

---

### 6ï¸âƒ£ Maximum Likelihood Estimation (MLE)
1. Write likelihood L(Î¸)=P(data | Î¸)  
2. Take log â†’ log-likelihood  
3. Differentiate w.r.t Î¸, set = 0  
4. Solve for Î¸Ì‚ (the MLE)

**Idea:** choose parameters that make observed data *most likely*.  
MLEs are consistent, unbiased (as nâ†’âˆ), and efficient.

---

### 7ï¸âƒ£ Confidence Intervals (CI)
- **Interpretation:** Repeating sampling many times, about 95 % of intervals contain the true parameter.  
- **Width increases** when:
  - Variability â†‘  
  - Confidence level â†‘  
  - Sample size â†“  
- **Analytical CI:** uses formulas (e.g., `xÌ„ Â± z * s/âˆšn`)  
- **Bootstrap CI:** obtained via resampling.

![success](Ci.png)

---

### 8ï¸âƒ£ Hypothesis Testing
1. Set up Hâ‚€ (null) and Hâ‚ (alternative).  
2. Compute test statistic (t, z, F, Ï‡Â²).  
3. Compute p-value = Pr(data or more extreme | Hâ‚€).  
4. Reject Hâ‚€ if p < Î± (usually 0.05).  

## ğŸ“ Inference and Hypothesis Testing Overview

Statistical inference helps us draw conclusions about populations using sample data.  
Depending on what we know (e.g., population variance, distributional assumptions, or sample design), we use **different inference methods**.

---

### ğŸ”¹ 1ï¸âƒ£ Z-Test vs. T-Test

Both the **z-test** and **t-test** are used to compare means or test hypotheses about population parameters.  
They differ mainly in what is **known** and **assumed** about the population variance (ÏƒÂ²) and sample size.

| Feature | **Z-Test** | **T-Test** |
|----------|-------------|------------|
| **Purpose** | Compare means (large samples or known Ïƒ) | Compare means (small samples, Ïƒ unknown) |
| **Population SD (Ïƒ)** | Known | Unknown (estimated from sample as s) |
| **Test Statistic** | `z = (xÌ„ âˆ’ Î¼â‚€) / (Ïƒ / âˆšn)` | `t = (xÌ„ âˆ’ Î¼â‚€) / (s / âˆšn)` |
| **Distribution Used** | Standard Normal (Z ~ N(0,1)) | Studentâ€™s t with (nâˆ’1) df |
| **Sample Size** | Large (n â‰¥ 30) | Small or moderate (n < 30) |
| **Assumptions** | Data ~ Normal or large n (by CLT) | Data ~ Normal or nearly Normal |
| **Shape of Sampling Dist.** | Fixed (does not depend on n) | Heavier tails (accounts for more uncertainty) |
| **As n increases** | Z always same | T â†’ Z (they converge) |

**When to use each:**
- âœ… **Z-test** â†’ population SD known or sample very large.  
- âœ… **T-test** â†’ population SD unknown (use sample SD); works for small or moderate n.

**Example Scenarios:**
| Scenario | Use | Reason |
|-----------|-----|--------|
| Testing average SAT score with known Ïƒ = 100 | Z-test | Ïƒ known |
| Comparing mean GPA of 25 students (Ïƒ unknown) | T-test | Ïƒ estimated from sample |
| Comparing average income between two independent groups | Two-sample T-test | Ïƒ unknown |
| Comparing mean score before vs after intervention (same students) | Paired T-test | Ïƒ unknown, dependent samples |
| Testing proportion of defective parts in a large batch | Z-test for proportions | large n, known binomial SE formula |

---

### ğŸ”¹ 2ï¸âƒ£ Parametric vs. Simulation-Based Inference

There are two broad approaches to statistical inference â€” **parametric** and **simulation-based** â€” that differ in how they model uncertainty.

| Feature | **Parametric Inference** | **Simulation-Based Inference** |
|----------|--------------------------|--------------------------------|
| **Basis** | Theoretical probability distributions (e.g., Normal, t, Ï‡Â², F) | Data-driven resampling (no distribution assumptions) |
| **Examples** | t-test, z-test, ANOVA, regression inference | Bootstrap CIs, permutation (randomization) tests |
| **Assumptions** | Known or assumed sampling distribution | Minimal assumptions; rely on the data itself |
| **Computation** | Analytical formulas (uses standard errors) | Computational â€” simulate or resample repeatedly |
| **When to Use** | When population model and assumptions are reasonable | When sample size small, distribution unknown, or statistic complex |
| **Output** | Theoretical p-values, confidence intervals | Empirical p-values or bootstrap confidence intervals |

**Intuition:**  
- **Parametric inference** = theoretical; relies on math formulas and distribution assumptions.  
- **Simulation-based inference** = empirical; approximates the sampling distribution by resampling from the observed data.  

**Examples:**
- **t-test:** parametric method using the t-distribution.  
- **Permutation test:** shuffle group labels under Hâ‚€ and recompute statistic many times.  
- **Bootstrap CI:** resample (with replacement) to estimate variability of the statistic.

---

### ğŸ”¹ 3ï¸âƒ£ Paired vs. Independent Samples

A **two-sample test** compares means or medians between two groups.  
Which test you use depends on whether the samples are **independent** or **paired (dependent)**.

| Situation | **Relationship Between Samples** | **Appropriate Test** | **Example** |
|------------|----------------------------------|----------------------|--------------|
| **Independent Samples** | Observations in one group are completely unrelated to those in the other | Independent Two-Sample T-Test (or Welchâ€™s T-Test if unequal variances) | Compare average GPA of students in two different classes |
| **Dependent / Paired Samples** | Same individuals measured twice, or matched pairs (twins, before-after) | Paired T-Test (based on differences within pairs) | Compare blood pressure before vs after medication for same patients |
| **Nonparametric Alternative (small n / non-normal)** | May use Wilcoxon Rank-Sum (independent) or Wilcoxon Signed-Rank (paired) | Nonparametric Test | Compare median scores for skewed data |

**Key Concepts:**
- **Independent test:** between-subject design â†’ differences across groups.  
- **Paired test:** within-subject design â†’ differences within the same individuals.  
- Pairing reduces variability because each subject serves as their own control.

---

### ğŸ§  Quick Conceptual Summary

| Concept | Definition | Example / Key Idea |
|----------|-------------|--------------------|
| **Z-Test** | Used when Ïƒ known or large n | Compare sample mean to population mean (known Ïƒ) |
| **T-Test** | Used when Ïƒ unknown | Compare means with estimated variance |
| **Parametric Inference** | Based on theoretical distributions | t-test, ANOVA, regression F-test |
| **Simulation-Based Inference** | Uses data resampling instead of assumptions | Permutation or bootstrap tests |
| **Paired Test** | Same individuals measured twice | Beforeâ€“after, matched pairs |
| **Independent Test** | Two unrelated groups | Two-sample comparison |

---

---

## ğŸ“ Unit 2: Linear Regression

---

### 1ï¸âƒ£ Simple and Multiple Linear Regression
**Model:**  
`yáµ¢ = Î²â‚€ + Î²â‚xáµ¢â‚ + Î²â‚‚xáµ¢â‚‚ + â€¦ + Î²â‚šxáµ¢â‚š + Îµáµ¢`

**Goal:** estimate coefficients Î² that minimize  
`RSS = Î£(yáµ¢ âˆ’ Å·áµ¢)Â²`

**Interpretation:**  
- Î²Ì‚â±¼ = expected change in Y for a one-unit change in Xâ±¼, holding others constant.  
- p-value tests if Î²â±¼ significantly differs from 0.  
- CI gives a range of plausible slopes.

---

### 2ï¸âƒ£ Matrix Notation
`Y = XÎ² + Îµ`  
- Y: nÃ—1 vector of responses  
- X: nÃ—p design matrix (first column = 1s for intercept)  
- Î²: pÃ—1 vector of coefficients  
- Îµ: nÃ—1 vector of errors  

**OLS solution:** `Î²Ì‚ = (Xáµ€X)â»Â¹Xáµ€Y`

---

### 3ï¸âƒ£ Categorical Predictors
- Represent each level (except one) with dummy variables (0/1).  
- Reference = baseline category.  
- For K levels â†’ K âˆ’ 1 dummies.  
- Avoid K dummies â†’ perfect multicollinearity.  

**Interpretation:** each coefficient = difference from reference group mean (Y).

---

### 4ï¸âƒ£ Interaction Terms
Model:  
`Y = Î²â‚€ + Î²â‚Xâ‚ + Î²â‚‚Xâ‚‚ + Î²â‚ƒ(Xâ‚Â·Xâ‚‚) + Îµ`

- The term Î²â‚ƒ(Xâ‚Â·Xâ‚‚) allows the effect of Xâ‚ to depend on Xâ‚‚.  
- If Î²â‚ƒ â‰  0 â†’ interaction significant.  
- Example: effect of dosage on anxiety differs by age group.  
- Use when theory or EDA suggest different slopes by groups.

---

### 5ï¸âƒ£ Model Assessment
**Coefficient of Determination (RÂ²):**  
`RÂ² = 1 âˆ’ (RSS / TSS)` â†’ proportion of variance in Y explained by model.

**Adjusted RÂ²:**  
`RÂ²_adj = 1 âˆ’ (1 âˆ’ RÂ²) * ((n âˆ’ 1)/(n âˆ’ p âˆ’ 1))`  
Penalizes extra predictors.

**Interpretation:**  
- High RÂ² means more variance explained but not necessarily better fit.  
- Use Adjusted RÂ² for comparing models with different numbers of predictors.

---

### 6ï¸âƒ£ Regression Assumptions

| Assumption | Diagnostic Plot | Expected | Violation / Fix |
|-------------|----------------|-----------|----------------|
| **Linearity** | Residuals vs Fitted | Random scatter | Add polynomial terms or transform X |
| **Independence** | Residuals vs Index | No trend | Account for design or use mixed models |
| **Normality** | Q-Q plot | Points â‰ˆ 45Â° line | Transform Y or use robust methods |
| **Equal Variance** | Residuals vs Fitted | Constant spread | Transform Y or weighted LS |

Violating assumptions affects inference (p-values, CIs) more than prediction.

---

### 7ï¸âƒ£ F-Test (Overall Model)
Tests if model explains any variation in Y.

- **Null:** Î²â‚ = Î²â‚‚ = â€¦ = Î²â‚š = 0  
- **Statistic:** `F = (SSR/p) / (SSE/(n âˆ’ p âˆ’ 1))`  
- **Decision:** large F â†’ reject Hâ‚€ â†’ at least one predictor significant.

---

### 8ï¸âƒ£ Nested F-Test (Partial F-Test)
Compares two models:

- Reduced model âŠ‚ Full model  
- Tests if added predictors significantly improve fit.

**Formula:**  
`F = ((RSS_R âˆ’ RSS_F)/(p_F âˆ’ p_R)) / (RSS_F/(n âˆ’ p_F))`

**Interpretation:**  
Reject Hâ‚€ if F large â†’ extra predictors help.

**Example (R):**
```r
anova(reduced_model, full_model)
```

---

### 9ï¸âƒ£ Influential Points and Diagnostics

#### a. Leverage
- **Definition:** Measures how far a pointâ€™s predictor values are from the average of all predictors.  
- **Formula (conceptually):**  
  `háµ¢áµ¢ = xáµ¢áµ€ (Xáµ€X)â»Â¹ xáµ¢`  
- **Interpretation:**  
  - High leverage â†’ point is far out in X-space.  
  - These points have the *potential* to influence the regression line, even if their residual is small.  
  - The average leverage = (p + 1)/n, where p = number of predictors.  
- **Guideline:**  
  - háµ¢áµ¢ > 2Ã—average â†’ moderately high leverage  
  - háµ¢áµ¢ > 3Ã—average â†’ very high leverage  

---

#### b. Residuals vs Leverage Plot
- **Purpose:** identify points that combine *high leverage* (far in X-space) and *large residuals* (far in Y).  
- **Axes:**  
  - x-axis â†’ leverage  
  - y-axis â†’ studentized (standardized) residuals  
- **Interpretation:**  
  - Points near the center: typical observations.  
  - Points far right â†’ high leverage.  
  - Points far up/down â†’ large residuals (Y-outliers).  
  - Points far right *and* outside the Cookâ€™s-distance contours â†’ **influential**.  

---

#### c. Cookâ€™s Distance
- **Definition:** Quantifies how much the entire fitted model would change if an observation were removed.  
- **Approximate formula:**  
  `Dáµ¢ = (eáµ¢Â² / (pÂ·MSE)) * (háµ¢áµ¢ / (1 âˆ’ háµ¢áµ¢)Â²)`  
- **Interpretation:**  
  - Dáµ¢ â‰ˆ 0 â†’ no influence  
  - Dáµ¢ > 0.5 â†’ potentially influential  
  - Dáµ¢ > 1 â†’ highly influential  
- **What to do:**  
  - Verify data entry.  
  - Fit the model with and without the observation â€” if coefficients change greatly, document or justify exclusion.  

---

#### d. Variance Inflation Factor (VIF)
- **Definition:** Measures how much a coefficientâ€™s variance is inflated due to collinearity.  
- **Formula:**  
  `VIFâ±¼ = 1 / (1 âˆ’ Râ±¼Â²)`  
  where Râ±¼Â² = RÂ² from regressing predictor Xâ±¼ on all other Xâ€™s.  
- **Guidelines:**  
  - VIF â‰ˆ 1 â†’ no correlation  
  - VIF > 5 â†’ moderate multicollinearity  
  - VIF > 10 â†’ serious multicollinearity  
- **Fixes:**  
  - Remove redundant predictors.  
  - Combine correlated features.  
  - Use ridge regression or PCA if all predictors are important.  

---

### ğŸ”Ÿ Multicollinearity

- **Definition:** Two or more predictors are highly linearly related.  
- **Why it matters:** makes coefficients unstable and inflates their standard errors.  

**Consequences**
- Coefficient signs may flip unexpectedly.  
- Large SE â†’ insignificant p-values even for important variables.  
- Difficult to isolate the individual effect of predictors.  

**Detection**
- High pairwise correlations (|r| > 0.9).  
- High VIFs (see above).  

**Solutions**
- Drop or combine correlated variables.  
- Standardize predictors to improve interpretability.  
- Apply regularization (ridge / lasso).  
- In some cases, accept collinearity if predictors are theoretically essential.  

---

### ğŸ“Š Diagnostic Plot Summary

| Plot | What It Checks | Expected Pattern | Violation Indicates |
|------|----------------|------------------|----------------------|
| **Residuals vs Fitted** | Linearity + Equal Variance | Random scatter around 0 | Curved pattern â†’ nonlinearity; Funnel â†’ heteroscedasticity |
| **Qâ€“Q Plot** | Normality of residuals | Points â‰ˆ 45Â° line | Skewed or heavy tails â†’ non-normal errors |
| **Scaleâ€“Location** | Homoscedasticity | Horizontal band | Funnel â†’ non-constant variance |
| **Residuals vs Leverage** | Influence / Outliers | Most within Cookâ€™s D < 1 | Outliers or high leverage points |

---

### ğŸ§  Concept and Test Relationships

| Concept | Purpose | Formula / Key Idea |
|----------|----------|--------------------|
| **t-Test** | Tests a single coefficient | `t = Î²Ì‚ / SE(Î²Ì‚)` |
| **F-Test (overall)** | Tests if *any* predictor matters | `F = MSR / MSE` |
| **Nested F-Test** | Tests if new predictors improve model | `F = ((RSS_R âˆ’ RSS_F)/(p_F âˆ’ p_R)) / (RSS_F/(n âˆ’ p_F))` |
| **Leverage** | Measures distance in X-space | `háµ¢áµ¢` |
| **Cookâ€™s Distance** | Measures influence | `Dáµ¢` |
| **VIF** | Detects multicollinearity | `1 / (1 âˆ’ RÂ²)` |

---

### ğŸ§¾ Practical Exam Tips

**Focus on interpretation rather than memorization**
- Understand what each test or plot *tells you*.  
- Be able to read regression output:  
  - Which coefficients are significant?  
  - What is the practical meaning of Î²Ì‚â±¼?  
  - Are assumptions violated (based on residual plots)?  

**Key ideas to recall**
- **CLT:** sampling distributions become approximately normal.  
- **OLS:** minimizes RSS â†’ best linear unbiased estimates (BLUE).  
- **F-ratio:** compares explained vs. unexplained variance.  
- **Nested F-test:** compares two models directly.  

**Quick thresholds**
- p < 0.05 â†’ statistically significant.  
- Dáµ¢ > 1 â†’ potentially influential.  
- VIF > 5 â†’ collinearity concern.  

**Strategy for short-answer questions**
1. Identify model type (SLR / MLR).  
2. Recall what the coefficient or statistic represents.  
3. State direction (+/â€“) and magnitude of effect.  
4. Comment on assumption checks (plots).  

**Remember:**  
- Interpret regression *in context* (e.g., units, variables).  
- A large RÂ² doesnâ€™t mean the model is valid â†’ check residual diagnostics.  
- High leverage â‰  automatically bad â€” only problematic if residuals are large too.  

---

### ğŸ§­ Final Summary â€” Top Exam Takeaways

1. **Understand the 4 regression assumptions** (linearity, independence, normality, homoscedasticity).  
2. **OLS minimizes squared residuals** to estimate Î²â€™s.  
3. **t-tests** check individual predictors; **F-tests** check groups.  
4. **Nested F-tests** compare reduced vs full models.  
5. **RÂ² vs Adjusted RÂ²:** adjusted penalizes extra predictors.  
6. **Leverage + Cookâ€™s Distance:** detect influence.  
7. **VIF:** detect multicollinearity.  
8. **Confidence Intervals:** interpretation = â€œif repeated many times, 95 % of CIs would contain Î².â€  
9. **p-values:** probability of data (or more extreme) given Hâ‚€ true.  
10. **Always explain in plain language** â€” what does the coefficient or diagnostic mean in the studyâ€™s context?  

---


